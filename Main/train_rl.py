# train_rl.py
"""
Train a MaskablePPO agent to select the seed set that maximises
NSGA-II hyper-volume.  Checkpoints are saved in the `runs/` folder.
"""

import os, pickle
from sb3_contrib import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker
import seed_env
from seed_env import SeedEnv, action_mask   # action_mask comes from seed_env


DEBUG = True                 # keep using the debug branch

if DEBUG:
    PICKS       = 100        # 100 molecules per episode
    N_GEN       = 30         # 30 GA generations
    TOTAL_STEPS = 20_000     # ≈ 200 episodes  (100 × 30 GA runs ≈ 5 min)
    SAVE_EVERY  = 5_000
else:
    PICKS       = 200
    N_GEN       = 100
    TOTAL_STEPS = 200_000
    SAVE_EVERY  = 20_000

POOL_FILE = "pool_with_props.pkl"   # generated by make_pool_props.py
RUNS_DIR  = "runs"


def main():
    # 1) load pool and property table
    pool, props = pickle.load(open(POOL_FILE, "rb"))

    # 2) build masked environment
    base_env = SeedEnv(pool, props, picks=PICKS, n_gen=N_GEN)
    env = ActionMasker(base_env, action_mask)   # masks invalid indices

    # 3) create MaskablePPO model
    model = MaskablePPO(
        "MlpPolicy",
        env,
        verbose=1,
        n_steps=256,          # small rollout for quick feedback
        batch_size=256,
        tensorboard_log=os.path.join(RUNS_DIR, "tb"),
    )

    # 4) training loop with checkpoints
    timesteps = 0
    while timesteps < TOTAL_STEPS:
        next_stop = min(timesteps + SAVE_EVERY, TOTAL_STEPS)
        model.learn(total_timesteps=next_stop - timesteps, reset_num_timesteps=False)
        timesteps = next_stop

        ckpt = os.path.join(RUNS_DIR, f"ppo_seedenv_{timesteps}.zip")
        model.save(ckpt)
        print(f"Checkpoint saved → {ckpt}")

    # 5) quick post-training evaluation (2 episodes for speed)
    rewards = []
    for _ in range(2):
        obs, _ = env.reset()
        done = False
        while not done:
            # ---- get current mask and pass it to predict() ----
            mask = seed_env.action_mask(base_env)  # boolean 1×N
            action, _ = model.predict(
                obs, deterministic=True, action_masks=mask
            )
            obs, r, done, _, _ = env.step(int(action))
        rewards.append(r)

    print("\n=== Evaluation ===")
    print("mean HV :", sum(rewards) / len(rewards))
    print("rewards :", rewards)


if __name__ == "__main__":
    os.makedirs(RUNS_DIR, exist_ok=True)
    main()
